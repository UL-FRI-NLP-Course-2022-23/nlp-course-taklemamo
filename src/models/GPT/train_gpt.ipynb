{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1337\n",
    "\n",
    "checkpoint = \"cjvt/gpt-sl-base\"\n",
    "max_len = 512 # max tokens in sentence / paraphrase -> model input: max_len*2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, random_split\n",
    "import pandas as pd\n",
    "\n",
    "class ParaDataset(Dataset):\n",
    "\n",
    "  def __init__(self, fpath, tokenizer, max_len=512*2):\n",
    "    super().__init__()\n",
    "    self.raw_data = self._load(fpath)\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "\n",
    "    # hacky shit\n",
    "    # problem is that with this tokenizer <SEP> is mapped to <EOS>\n",
    "    # making it unusable\n",
    "    # adding new special token is not an option since\n",
    "    # that requires model retraining (tokenizer changes completely)\n",
    "    self.sep = \"==>\"\n",
    "    self.sep_ids = self.tokenizer([self.sep])[\"input_ids\"][0][1:]\n",
    "    \n",
    "    self.inputs, self.labels = self._preprocess(self.raw_data)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.raw_data)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    out = {k:v[index] for k,v in self.inputs.items()}\n",
    "    out[\"labels\"] = self.labels[index]\n",
    "    return out\n",
    "\n",
    "  def _load(self, fpath):\n",
    "    return pd.read_csv(fpath, sep=\"\\t\", names=[\"paragraph\", \"paraphrase\"])\n",
    "\n",
    "  def _preprocess(self, raw_data):\n",
    "    inputs = self.tokenizer([\n",
    "        \"<BOS>\" + paragraph + self.sep + paraphrase + \"<EOS>\" # tokenizer doesn't seem to be adding <BOS> and <EOS>\n",
    "        for paragraph, paraphrase in zip(raw_data.paragraph, raw_data.paraphrase)\n",
    "    ], truncation=True, padding=\"max_length\", max_length=self.max_len)\n",
    "\n",
    "    # manually construct labels\n",
    "    # shifting to left is done inside model during training\n",
    "    # so labels should be the same as inputs\n",
    "    labels = []\n",
    "    for input_ids in inputs[\"input_ids\"]:\n",
    "      label = input_ids.copy()\n",
    "      for i in range(len(label)):\n",
    "        if label[i:i+len(self.sep_ids)] == self.sep_ids:\n",
    "          sep_pos = i\n",
    "        if label[i] == 0:\n",
    "          label[i] = -100 # mask padding\n",
    "      label[1:sep_pos] = [-100]*(sep_pos-1) # mask input sentence\n",
    "      labels.append(label)\n",
    "\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs</th>\n",
       "      <th>targets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amsterdam - Le nekaj mesecev potem, ko so nizo...</td>\n",
       "      <td>Amsterdam - Le nekaj mesecev po tem, ko so niz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"S trenerjem sva načrtovala uvrstitev v najbol...</td>\n",
       "      <td>\"S trenerjem sva načrtovala uvrstitev med najb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Najprej zato, ker znajo gledalcem, ki se jih j...</td>\n",
       "      <td>Najprej zato, ker znajo gledalcem ponuditi, ki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Izidi: 1. kolo - skupina A: ZRJ - Grčija 83:72...</td>\n",
       "      <td>: Rezultati 1 kolo - skupina A: FRY - Grčija 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tekmovanje se bo pravzaprav začelo že danes z ...</td>\n",
       "      <td>Tekmovanje se bo pravzaprav začelo danes z ura...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11306</th>\n",
       "      <td>Bistvo vsega ni naše telo, temveč telo tehnolo...</td>\n",
       "      <td>Bistvo vsega ni naše telo, ampak telo tehnolog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11307</th>\n",
       "      <td>Crowley je bil tudi sam umetnik. Za njim je os...</td>\n",
       "      <td>Crowley sam je bil umetnik, ki je zapustil pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11308</th>\n",
       "      <td>Vsi, ki jih \"prerok Horusovega eona\" tako ali ...</td>\n",
       "      <td>Vsi, ki so na tak ali drugačen način zgleduje ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11309</th>\n",
       "      <td>Lib Demi, ki so obvladovali britansko političn...</td>\n",
       "      <td>Lib Demi, ki je prevladovala na britanski poli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11310</th>\n",
       "      <td>Jaz, robot - ravnokar polni naše kinodvorane -...</td>\n",
       "      <td>jaz, robot - ki pravkar polni naše kinematogra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11311 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  inputs   \n",
       "0      Amsterdam - Le nekaj mesecev potem, ko so nizo...  \\\n",
       "1      \"S trenerjem sva načrtovala uvrstitev v najbol...   \n",
       "2      Najprej zato, ker znajo gledalcem, ki se jih j...   \n",
       "3      Izidi: 1. kolo - skupina A: ZRJ - Grčija 83:72...   \n",
       "4      Tekmovanje se bo pravzaprav začelo že danes z ...   \n",
       "...                                                  ...   \n",
       "11306  Bistvo vsega ni naše telo, temveč telo tehnolo...   \n",
       "11307  Crowley je bil tudi sam umetnik. Za njim je os...   \n",
       "11308  Vsi, ki jih \"prerok Horusovega eona\" tako ali ...   \n",
       "11309  Lib Demi, ki so obvladovali britansko političn...   \n",
       "11310  Jaz, robot - ravnokar polni naše kinodvorane -...   \n",
       "\n",
       "                                                 targets  \n",
       "0      Amsterdam - Le nekaj mesecev po tem, ko so niz...  \n",
       "1      \"S trenerjem sva načrtovala uvrstitev med najb...  \n",
       "2      Najprej zato, ker znajo gledalcem ponuditi, ki...  \n",
       "3      : Rezultati 1 kolo - skupina A: FRY - Grčija 8...  \n",
       "4      Tekmovanje se bo pravzaprav začelo danes z ura...  \n",
       "...                                                  ...  \n",
       "11306  Bistvo vsega ni naše telo, ampak telo tehnolog...  \n",
       "11307  Crowley sam je bil umetnik, ki je zapustil pre...  \n",
       "11308  Vsi, ki so na tak ali drugačen način zgleduje ...  \n",
       "11309  Lib Demi, ki je prevladovala na britanski poli...  \n",
       "11310  jaz, robot - ki pravkar polni naše kinematogra...  \n",
       "\n",
       "[11311 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = \"../../../data/backtranslate/backtranslate.csv\"\n",
    "\n",
    "data = pd.read_csv(dataset_path, sep=\"\\t\", names=[\"inputs\", \"targets\"])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "paraset = ParaDataset(dataset_path, tokenizer, max_len*2)\n",
    "\n",
    "gen = torch.Generator().manual_seed(SEED)\n",
    "train_set, val_set = random_split(paraset, [0.9, 0.1], generator=gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir= \"./gpt\",\n",
    "    overwrite_output_dir=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=1,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=val_set,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
